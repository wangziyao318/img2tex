\documentclass[acmtog]{acmart}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}
% \copyrightyear{2024}
% \acmYear{2024}
% \acmDOI{XXXXXXX.XXXXXXX}


%%
%% These commands are for a JOURNAL article.
\acmJournal{TOG}
\acmVolume{X}
\acmNumber{X}
\acmArticle{X}
\acmMonth{12}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{3D Human Texture Estimation from Single Image}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ziyao Wang}
\affiliation{%
  \institution{University of British Columbia}
  \city{Vancouver}
  \state{BC}
  \country{Canada}
}
\email{zwubc@ece.ubc.ca}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Ziyao Wang}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  The paper SMPLitex \cite{casas2023smplitex} proposed a generative model for 3D human texture estimation from single image. SMPLitex is built on the stable diffusion model for image generation and the UV mapping technique for 2D representation of 3D texture. SMPLitex itself refers to a fine-tuned stable diffusion model that can generate an unwrapped UV map from text description directly, or from a single image after preprocessing. In this paper, we extend SMPLitex in two directions. On one hand, we integrate all the preprocessing and generating steps into a pipeline that accepts text description or a single image as input and produces an unwrapped UV map as output. On the other hand, we also fine-tune the stable diffusion v3 model with more training data from public datasets and compare it quantitatively with the original SMPLitex model using SSIM and LPIPS metrics.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10010147.10010178.10010224.10010245.10010254</concept_id>
  <concept_desc>Computing methodologies~Reconstruction</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Reconstruction}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Stable Diffusion, SMPL human model, DensePose, Sematic Segmentation}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
The aim of the project is to extend the paper SMPLitex \cite{casas2023smplitex} by first integrating all steps into a pipeline and then fine-tuning stable diffusion model to compare with the SMPLitex model using SSIM and LPIPS.

\subsection{SMPLitex}

The paper SMPLitex \cite{casas2023smplitex} proposed a generative model for 3D human texture estimation from single image. SMPLitex is based on the stable diffusion model for image generation and the UV mapping technique for 2D representation of 3D texture. SMPLitex itself refers to a fine-tuned stable diffusion model that can generate an unwrapped UV map from text description directly, or from a single image after three preprocessing steps including DensePose from Detectron2, Semantic Guided Human Matting, and UVTextureConverter.

The main contribution of SMPLitex is to connect 2D image of human to 3D texture of human model. A single image of human taken from the front lacks certain perspectives from the back and sides due to blocking, and therefore only a partial UV map can be created from it accurately. Nonetheless, with the help of generative model like stable diffusion, holes and blank parts in the partial UV map can be generated and filled based on the knowledge learnt from training data. The SMPLitex model was inspired by this idea.

\subsection{Stable Diffusion}
Stable diffusion is a text-to-image model that can generate images from pure noise using text-guided denoising steps. A stable diffusion model has three components: variational autoencoders/autodecoders (VAE),  a U-Net, and a text encoder CLIP.

\begin{itemize}
  \item VAE produces input varied in dimension as output. VAE encoders reduce the dimension of input, and VAE decoders increase the dimension of input. A 512x512 image has 3x512x512 dimensions (3 for RGB channels), which will result in curse of dimensionality without proper encoding.
  \item CLIP encodes the text input as text embedding vectors to use in U-Net.
  \item U-Net performs denoising guided by text embeddings. And training stable diffusion model is actually training the U-Net in it.
\end{itemize}

A randomly-generated pure noise image first goes through several VAE encoders to reduce dimensionality, and then is denoised with a trained U-Net guided by CLIP-encoded text embeddings. Finally, the output image goes through several VAE decoders to restore its dimentionality.

It's worth mentioning that the image generated by stable diffusion model is quite random, which leads us to fine-tuned stable diffusion model \cite{ruiz2023dreambooth} that is constrained to synthesis only a certain type of images like SMPL UV maps. SMPLitex model is fine-tuned using 10 samples over 1500 iterations on stable diffusion LDM model.

\subsection{UV Mapping}

UV maps provide a mapping of each 3D mesh vertex of human model into 2D image space by unwrapping the 3D mesh surface. This technique allows to map image color information onto the 3D model at a high quality even on low-resolution meshes.

Each type of mesh surface has a specific arrangement of its UV map. Considering 3D human, SMPL model is open-source and widely acknowledged. Thereby, its arrangement of UV map is used in our project.

\section{Related Work}
Besides SMPLitex \cite{casas2023smplitex}, previous work goes to preprocessing steps of 2D image and fine-tuning stable diffusion models.

\begin{description}
  \item[DensePose] (dense human pose estimation) \cite{guler2018densepose} aims at mapping all human pixels of an 2D image to the 3D surface of the human body. It is such important that it connects a 2D image to a partial UV map and assign pixels accurately to different human parts. Like SMPL, DensePose also has its UV map. But, with higher resolution and more parts, DensePose partial UV map is harder to recover than SMPL ones using stable diffusion models.
  \item[Semantic Guided Human Matting] \cite{chen2022robust} is a semantic segmentation model specifically trained for human. It is very robust and accurate when it comes to removing the background of a portrait. It is used together with DensePose to generate DensePose partial UV map from 2D image.
  \item[UVTextureConverter] is a Python package to convert DensePose UV map to SMPL UV map.
  \item[DreamBooth] \cite{ruiz2023dreambooth} is a technique to quickly fine-tune stable diffusion models given very few training data. The aim is to constraint the model to generate only certain style of images, like SMPL UV map.
\end{description}

\section{Methods}

The methods used in the paper is for quantitative comparision of our trained model with SMPLitex model. To do so, we use SSIM and LPIPS.

\subsection{SSIM}
SSIM (structural similarity index measure) is a well-known method to measure structural similarity of two images. Unlike the common MSE (mean square error), SSIM focuses on the inter-dependency of pixels close to each other, which carries the structure information of the image.

Here's the formula of SSIM, where \(\mu\) represents pixel sample mean, \(\sigma_{xy}\) is covariance, and constant \(c_1\) and \(c_2\) help smooth the SSIM value and avoid zero division.

\[
  SSIM(x,y)=\frac{(2\mu_x\mu_y+c_1)(2\sigma_{xy}+c2)}{({\mu_x}^2+{\mu_y}^2+c_1)({\sigma_x}^2+{\sigma_y}^2+c_2)}
\]

SSIM score is similar to human perception. The range of SSIM is \([-1,1]\), where 0 indicates no similarity, 1 is structural identical, and -1 is perfect anti-correlation.

However, SSIM is vulnerable to compression, so it may not perform well when comparing compressed JPGs obtained from the Internet.

\subsection{LPIPS}
LPIPS (learned perceptual image patch similarity) \cite{zhang2018unreasonable} is a deep-learning based metric to compare two images perceptually just like human. It provides a Python package and helper methods ready to use in our project. Higher score means more different, and lower score (approx 0) means more similar perceptually.



\section{Progress to Date}

We have gone through the processes in SMPLitex paper to generate a SMPL UV map from a single image. After that, we have integrated these processes (DensePose -> Semantic Guided Human Matting -> UVTextureConverter) into a pipeline. The original processes take over 1 minute to generate a UV map and require lots of CLI commands and files copy and paste. Now it takes only 10s to generate a 512x512 SMPL UV map. I think why the author didn't do the integration is because code copyright issue may affect publication. But for me it's just a course project and doing so will greatly save my time later in results comparison.

The main contribution of the paper should be fine-tuning our model, and integration of the pipeline is just for us to familiarize the whole process and there's not much to document. We noticed the use of deprecated functions and outdated Python packages like \texttt{smplx} and \texttt{chumpy} and replaced them with newer alternatives like \texttt{smplkit}. We also noticed the incompatibility of Python package versions across different works (for example, Facebook's Detectron2 support only up to CUDA11.3) and managed to find a proper version to suit them all.

\section{Plan for Completion}

The next steps are fine-tuning and results comparison.

For training, we have RTX4090 GPU that is sufficient to train using DreamBooth on stable diffusion v3 model with 50+ training samples. A typical training time of 10 samples over 1500 iterations is half an hour, so it's feasible in time for us to train with more samples.

For results generation and comparison, we plan to use 3 test datasets: DeepFashion-MultiModal \cite{jiang2022text2human}, THUman2.0 \cite{yu2021function4d}, Market-1501 \cite{zheng2015scalable}, each of them we randomly take 10 samples. Generating one result using our integrated pipeline takes less than 10 seconds. For evaluation metrics SSIM and LPIPS, we use Python packages with their names.

We are confident to complete our project on time.

\section{Fine-tuning}

hyper-parameters used and explainations

possible mentions of errors and overcomes, tricks used in tuning, etc.

training datasets: SMPLitex dataset with 250 samples generated by SMPLitex model and selected to get rid of distortion.

\section{Results}

Evaluate on three datasets, each with some samples.

DeepFashion-MultiModal dataset \cite{jiang2022text2human}

THUman2.0 dataset \cite{yu2021function4d}

Market-1501 dataset \cite{zheng2015scalable}

Measure by SSIM and LPIPS metrics, on SMPLitex generated vs. ground truth and our generated vs. ground truth. Create a table and graphs.


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{report}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Sample Code Blocks}

Sample code blocks that help explaination will be inserted here as appendices.

\end{document}
\endinput
